{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x1_data = [73., 93., 89., 96., 73.]\n",
    "x2_data = [80., 88., 91., 98., 66.]\n",
    "x3_data = [75., 93., 90., 100., 70.]\n",
    "y_data = [152., 185., 180., 196., 142.]\n",
    "'''y는 CNN의 정답레이블 t에 대응된다고 생각하면 된다.\n",
    "이 예제에서는 MSE를 이용하게 되는데, \n",
    "CNN에서의 MSE는 신경망의 출력 y에 정답 레이블 t를 빼서 오차를 구하게 된다.\n",
    "여기서는 손실 함수의 개념이 아직 적용되지 않았고\n",
    "classification이 아니라 regression이므로, y 데이터가 저런 모양이다.'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x1 = tf.placeholder(tf.float32)\n",
    "x2 = tf.placeholder(tf.float32)\n",
    "x3 = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w1 = tf.Variable(tf.random_normal([1]), name='weight1')\n",
    "w2 = tf.Variable(tf.random_normal([1]), name='weight2')\n",
    "w3 = tf.Variable(tf.random_normal([1]), name='weight3')\n",
    "'''여기서 w1 w2 w3은 각 layer를 의미하는게 아니라, x batch(x1 x2 x3)에 각각 적용됨.\n",
    "w가 [1]이니까, w1의 값 1개가 모든 x1의 원소에 적용된다. \n",
    "원래 CNN에서는 x1의 원소 하나 당 w1의 원소 하나가 필요했고,\n",
    "이런 식으로 x1 당 한 개가 필요한 것은 b였는데, 여기서는 w도 그렇다.'''\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "hypothesis = x1 * w1 + x2 * w2 + x3 * w3 + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndd\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "#위에서 설명한 대로 hypothesis(출력)에서 Y를 뺀다.\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)\n",
    "train = optimizer.minimize(cost)\n",
    "'''\n",
    "DLfromScratch에서는 반복 마다 계산된 grads를 optimizer.update(params, grads)로 집어 넣어\n",
    "params를 갱신하도록 했다. \n",
    "여기서는 cost만 집어 넣으면 cost-hypothesis- w, b로 연결되어\n",
    "sess.run() 시 알아서 갱신되나봄.\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
